{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggaoshen/graph_wavenet/blob/sg-final/CS224W_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Final Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "Description goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J_m9l6OYCQZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31aa261b-48d9-417f-fc92-3e87ac1ed461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "  !pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qpr0ThDgZmZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f70420-16c2-4896-e1dc-1b4e160e3b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "12.1\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !nvcc --version\n",
        "  !python -c \"import torch; print(torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PRfgbfTjCRD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208b02de-d197-4b74-ccf2-3fb0ce7300b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "2.4.0\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  import torch\n",
        "  print(torch.__version__)\n",
        "  import torch_geometric\n",
        "  print(torch_geometric.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Re1G-ZlcQP",
        "outputId": "c33897f2-224f-43de-9566-5ad08d588349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'graph_wavenet'...\n",
            "remote: Enumerating objects: 198, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 198 (delta 17), reused 14 (delta 8), pack-reused 152\u001b[K\n",
            "Receiving objects: 100% (198/198), 163.30 KiB | 899.00 KiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n"
          ]
        }
      ],
      "source": [
        "# Import GraphWaveNet module\n",
        "!git clone -b sg-experiment https://github.com/ggaoshen/graph_wavenet.git # NOTE: choose the right branch\n",
        "%load graph_wavenet/src/graphwavenet.py\n",
        "import sys\n",
        "sys.path.append('graph_wavenet/src/')\n",
        "from graphwavenet import GraphWaveNet\n",
        "from util import masked_rmse, masked_mse, masked_mae, masked_mape, metric, temporal_dataset_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGgtAIg9lcQP",
        "outputId": "012c455b-d019-45d9-add6-daec77b370b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m747.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.1.0+cu121)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.0.6)\n",
            "Collecting pandas<=1.3.5 (from torch-geometric-temporal)\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (0.6.18+pt21cu121)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.1.2+pt21cu121)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch-geometric-temporal) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch-geometric-temporal) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-geometric-temporal) (2.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-geometric-temporal) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->torch-geometric-temporal) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Building wheels for collected packages: torch-geometric-temporal\n",
            "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=91b8e0f09160c65b93a69f8fd17e6c950532f4c164038485c8181b93cf545c6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
            "Successfully built torch-geometric-temporal\n",
            "Installing collected packages: pandas, torch-geometric-temporal\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.16.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.3.5 torch-geometric-temporal-0.54.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "\n",
        "\n",
        "# Temporal Datasets\n",
        "\n",
        "from torch_geometric_temporal.dataset import METRLADatasetLoader\n",
        "loader = METRLADatasetLoader()\n",
        "dataset = loader.get_dataset(num_timesteps_in=12, num_timesteps_out=12)\n",
        "iterator = iter(dataset)\n",
        "print(\"METRLA dataset from original Graph Wavenet paper: \\n\", next(iterator))\n",
        "\n",
        "\n",
        "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
        "dataset = ChickenpoxDatasetLoader().get_dataset(lags=8) # consistent with chickenpox paper\n",
        "iterator = iter(dataset)\n",
        "print(\"Chickenpox dataset: \\n\", next(iterator))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHipsw6jF91T",
        "outputId": "9b0ba3e0-9535-4926-d603-1ca1ba8752f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METRLA dataset from original Graph Wavenet paper: \n",
            " Data(x=[207, 2, 12], edge_index=[2, 1722], edge_attr=[1722], y=[207, 12])\n",
            "Chickenpox dataset: \n",
            " Data(x=[20, 8], edge_index=[2, 102], edge_attr=[102], y=[20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Chickenpox Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# training\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "save_path = \"store/checkpoint\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# Get data\n",
        "\n",
        "# from torch_geometric_temporal.signal import temporal_signal_split\n",
        "train_ratio, val_ratio = 0.8, 0.1\n",
        "# train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=train_ratio)\n",
        "train_dataset, val_dataset, test_dataset = temporal_dataset_split(dataset, train_split = train_ratio, validation_split = val_ratio)\n",
        "val_offset = int(dataset.snapshot_count * train_ratio) # starting index for valid set\n",
        "test_offset = val_offset+int(dataset.snapshot_count * val_ratio) # starting index for valid set\n",
        "\n",
        "def prepare_n_period_y(dataset):\n",
        "\n",
        "    res = []\n",
        "    for data in dataset:\n",
        "        res.append(data.y)\n",
        "    res = torch.stack(res, dim=0)\n",
        "\n",
        "    return res\n",
        "\n",
        "y_all = prepare_n_period_y(dataset)\n",
        "\n",
        "\n",
        "# Model inputs\n",
        "in_dim = dataset[0].num_node_features # 8 treat lagged inputs as node features\n",
        "out_dim = 1\n",
        "num_nodes = dataset[0].num_nodes # 1068\n",
        "timesteps_to_predict = 10 # 10, 20, 40 week forecast horizon\n",
        "epochs = 200\n",
        "lrate = 0.0001\n",
        "wdecay = 0.001\n",
        "\n",
        "# early stopping parameters\n",
        "patience = 10\n",
        "counter = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "\n",
        "model = GraphWaveNet(\n",
        "    num_nodes=num_nodes,\n",
        "    in_channels=in_dim,\n",
        "    out_channels=out_dim,\n",
        "    out_timesteps=timesteps_to_predict,\n",
        ").to(device)\n",
        "\n",
        "# Training loop\n",
        "print(\"start training...\", flush=True)\n",
        "his_loss = []\n",
        "val_time = []\n",
        "train_time = []\n",
        "best_epoch = 0\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "\n",
        "model.train()\n",
        "\n",
        "training_curve_dict = {\"epoch_train_loss\": [], \"epoch_valid_loss\": []}\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss = []\n",
        "    t1 = time.time()\n",
        "\n",
        "    for i, snapshot in enumerate(train_dataset):\n",
        "        x_train = snapshot.x.reshape(-1, num_nodes, in_dim).to(device)\n",
        "        if timesteps_to_predict == 1:\n",
        "            y_train = snapshot.y.to(device)\n",
        "        else:\n",
        "            y_train = y_all[i : i + timesteps_to_predict,:].to(device)\n",
        "\n",
        "        pred = model(x_train, snapshot.edge_index, snapshot.edge_attr).squeeze()\n",
        "        loss = masked_mse(pred, y_train, 0.0) # mean squared error for loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    mtrain_loss = np.mean(train_loss)\n",
        "    training_curve_dict['epoch_train_loss'].append(mtrain_loss)\n",
        "    print(f\"training loss: {mtrain_loss}\")\n",
        "\n",
        "\n",
        "    t2 = time.time()\n",
        "    train_time.append(t2 - t1)\n",
        "    valid_loss = []\n",
        "\n",
        "    s1 = time.time()\n",
        "    for i, snapshot in enumerate(val_dataset):\n",
        "        x_val = snapshot.x.reshape(-1, num_nodes, in_dim).to(device)\n",
        "        if timesteps_to_predict == 1:\n",
        "            y_val = snapshot.y.to(device)\n",
        "        else:\n",
        "            y_val = y_all[val_offset + i : val_offset + i + timesteps_to_predict,:].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(x_val, snapshot.edge_index, snapshot.edge_attr).squeeze()\n",
        "        loss = masked_mse(pred, y_val, 0.0).cpu().numpy()\n",
        "        valid_loss.append(loss)\n",
        "\n",
        "    s2 = time.time()\n",
        "    # log = \"Epoch: {:03d}, Inference Time: {:.4f} secs\"\n",
        "    # print(log.format(i, (s2 - s1)))\n",
        "    val_time.append(s2 - s1)\n",
        "\n",
        "    mvalid_loss = np.mean(valid_loss)\n",
        "    training_curve_dict['epoch_valid_loss'].append(mvalid_loss)\n",
        "\n",
        "    his_loss.append(mvalid_loss)\n",
        "\n",
        "    if np.argmin(his_loss) == len(his_loss) - 1:\n",
        "        torch.save(\n",
        "            model.state_dict(), save_path + \"/epoch_\" + str(i) + \".pth\"\n",
        "        )\n",
        "        best_epoch = i\n",
        "\n",
        "    log = (\n",
        "        \"Epoch: {:03d}, Train Loss: {:.4f}, \"\n",
        "        + \"Valid Loss: {:.4f}, \"\n",
        "        + \"Training Time: {:.4f}/epoch\"\n",
        "    )\n",
        "    print(\n",
        "        log.format(\n",
        "            i,\n",
        "            mtrain_loss,\n",
        "            mvalid_loss,\n",
        "            (t2 - t1),\n",
        "        ),\n",
        "        flush=True,\n",
        "    )\n",
        "print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
        "print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
        "\n",
        "\n",
        "# eval\n",
        "model.load_state_dict(\n",
        "    torch.load(save_path + \"/epoch_\" + str(best_epoch) + \".pth\")\n",
        ")\n",
        "model.eval()\n",
        "loss = 0\n",
        "for i, snapshot in enumerate(test_dataset[:-timesteps_to_predict]):\n",
        "\n",
        "    x_test = snapshot.x.reshape(-1, num_nodes, in_dim).to(device)\n",
        "\n",
        "    if timesteps_to_predict == 1:\n",
        "        y_test = snapshot.y.to(device)\n",
        "    else:\n",
        "        y_test = y_all[test_offset+i : test_offset+i + timesteps_to_predict,:].to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(x_test, snapshot.edge_index, snapshot.edge_attr).squeeze()\n",
        "    loss += masked_mse(pred, y_test, 0.0) # mean squared error as loss\n",
        "\n",
        "loss = loss / (i+1)\n",
        "loss = loss.item()\n",
        "print(\"Test MSE Loss: {:.4f}\".format(loss))\n",
        "\n",
        "\n",
        "# store training loss\n",
        "df = pd.DataFrame(training_curve_dict)\n",
        "df.to_csv(\"training_curve.csv\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('training_curve.csv')\n",
        "\n",
        "# plot training and validation loss\n",
        "df.plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW_Gc1oQ8rNC",
        "outputId": "1cf60549-355f-4d7e-d7c5-9b40ab83e72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9670065093149499\n",
            "Epoch: 050, Train Loss: 0.9670, Valid Loss: 0.8954, Training Time: 21.9304/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/200 [00:23<1:18:56, 23.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9522934666039741\n",
            "Epoch: 050, Train Loss: 0.9523, Valid Loss: 0.8916, Training Time: 22.5046/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/200 [00:47<1:18:40, 23.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9464662316823151\n",
            "Epoch: 050, Train Loss: 0.9465, Valid Loss: 0.8886, Training Time: 23.1274/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/200 [01:12<1:19:15, 24.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9428570809887676\n",
            "Epoch: 050, Train Loss: 0.9429, Valid Loss: 0.8875, Training Time: 23.0285/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/200 [01:36<1:19:11, 24.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9402506274012167\n",
            "Epoch: 050, Train Loss: 0.9403, Valid Loss: 0.8869, Training Time: 22.9700/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▎         | 5/200 [02:00<1:18:55, 24.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9385926656152417\n",
            "Epoch: 050, Train Loss: 0.9386, Valid Loss: 0.8873, Training Time: 22.2180/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/200 [02:25<1:18:19, 24.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9369149134108206\n",
            "Epoch: 050, Train Loss: 0.9369, Valid Loss: 0.8868, Training Time: 22.4944/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/200 [02:48<1:17:33, 24.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9358525120721357\n",
            "Epoch: 050, Train Loss: 0.9359, Valid Loss: 0.8856, Training Time: 23.3982/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/200 [03:13<1:17:53, 24.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9343417786243485\n",
            "Epoch: 050, Train Loss: 0.9343, Valid Loss: 0.8857, Training Time: 23.0488/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 9/200 [03:38<1:17:32, 24.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9336091564423064\n",
            "Epoch: 050, Train Loss: 0.9336, Valid Loss: 0.8859, Training Time: 23.0022/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/200 [04:02<1:17:08, 24.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9326754866077043\n",
            "Epoch: 050, Train Loss: 0.9327, Valid Loss: 0.8852, Training Time: 22.5446/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/200 [04:26<1:16:46, 24.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9323383237575976\n",
            "Epoch: 050, Train Loss: 0.9323, Valid Loss: 0.8856, Training Time: 22.2490/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/200 [04:50<1:15:54, 24.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9313087186480804\n",
            "Epoch: 050, Train Loss: 0.9313, Valid Loss: 0.8861, Training Time: 23.0610/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 13/200 [05:15<1:15:43, 24.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9311377667435785\n",
            "Epoch: 050, Train Loss: 0.9311, Valid Loss: 0.8858, Training Time: 23.2362/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/200 [05:39<1:15:36, 24.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9304937975254001\n",
            "Epoch: 050, Train Loss: 0.9305, Valid Loss: 0.8850, Training Time: 23.4287/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/200 [06:04<1:15:37, 24.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9302871889548331\n",
            "Epoch: 050, Train Loss: 0.9303, Valid Loss: 0.8853, Training Time: 24.0991/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 16/200 [06:30<1:16:05, 24.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 0.9300256919252073\n",
            "Epoch: 050, Train Loss: 0.9300, Valid Loss: 0.8848, Training Time: 24.1099/epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 17/200 [06:55<1:16:19, 25.03s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NcsS-FdiPrRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a04421-2eda-424e-f82c-58a5f1b79e30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset GPU RAM if GPU out of memory\n",
        "\n",
        "# !pip install numba\n",
        "\n",
        "# from numba import cuda\n",
        "\n",
        "# cuda.select_device(0) # choosing second GPU\n",
        "\n",
        "# cuda.close()"
      ],
      "metadata": {
        "id": "GU0OV1AJUVCT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fClspcuQO8E6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}