{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggaoshen/graph_wavenet/blob/main/CS224W_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Final Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "Description goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_m9l6OYCQZP"
      },
      "outputs": [],
      "source": [
        "# Install torch geometric\n",
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "  !pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpr0ThDgZmZV"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !nvcc --version\n",
        "  !python -c \"import torch; print(torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRfgbfTjCRD_"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  import torch\n",
        "  print(torch.__version__)\n",
        "  import torch_geometric\n",
        "  print(torch_geometric.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import GraphWaveNet module\n",
        "!git clone https://github.com/ggaoshen/graph_wavenet.git\n",
        "%load graph_wavenet/graph_wavenet/graphwavenet.py\n",
        "import sys\n",
        "sys.path.append('graph_wavenet/graph_wavenet/')  \n",
        "from graphwavenet import GraphWaveNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset from TGB\n",
        "name = \"tgbn-trade\"\n",
        "dataset = PyGNodePropPredDataset(name=name, root=\"datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TGB Example\n",
        "\n",
        "import timeit\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.loader import TemporalDataLoader\n",
        "from torch_geometric.nn import TGNMemory\n",
        "from torch_geometric.nn.models.tgn import (\n",
        "    IdentityMessage,\n",
        "    LastAggregator,\n",
        "    LastNeighborLoader,\n",
        ")\n",
        "\n",
        "from modules.decoder import NodePredictor\n",
        "from modules.emb_module import GraphAttentionEmbedding\n",
        "from tgb.nodeproppred.dataset_pyg import PyGNodePropPredDataset\n",
        "from tgb.nodeproppred.evaluate import Evaluator\n",
        "from tgb.utils.utils import set_random_seed\n",
        "from tgb.utils.stats import plot_curve\n",
        "\n",
        "parser = argparse.ArgumentParser(description='parsing command line arguments as hyperparameters')\n",
        "parser.add_argument('-s', '--seed', type=int, default=1,\n",
        "                    help='random seed to use')\n",
        "parser.parse_args()\n",
        "args = parser.parse_args()\n",
        "# setting random seed\n",
        "seed = int(args.seed) #1,2,3,4,5\n",
        "torch.manual_seed(seed)\n",
        "set_random_seed(seed)\n",
        "\n",
        "# hyperparameters\n",
        "lr = 0.0001\n",
        "epochs = 50\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "name = \"tgbn-trade\"\n",
        "dataset = PyGNodePropPredDataset(name=name, root=\"datasets\")\n",
        "train_mask = dataset.train_mask\n",
        "val_mask = dataset.val_mask\n",
        "test_mask = dataset.test_mask\n",
        "\n",
        "eval_metric = dataset.eval_metric\n",
        "num_classes = dataset.num_classes\n",
        "data = dataset.get_TemporalData()\n",
        "data = data.to(device)\n",
        "\n",
        "evaluator = Evaluator(name=name)\n",
        "\n",
        "\n",
        "train_data = data[train_mask]\n",
        "val_data = data[val_mask]\n",
        "test_data = data[test_mask]\n",
        "\n",
        "# Ensure to only sample actual destination nodes as negatives.\n",
        "min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
        "\n",
        "batch_size = 200\n",
        "\n",
        "train_loader = TemporalDataLoader(train_data, batch_size=batch_size)\n",
        "val_loader = TemporalDataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = TemporalDataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "neighbor_loader = LastNeighborLoader(data.num_nodes, size=10, device=device)\n",
        "\n",
        "memory_dim = time_dim = embedding_dim = 100\n",
        "\n",
        "memory = TGNMemory(\n",
        "    data.num_nodes,\n",
        "    data.msg.size(-1),\n",
        "    memory_dim,\n",
        "    time_dim,\n",
        "    message_module=IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
        "    aggregator_module=LastAggregator(),\n",
        ").to(device)\n",
        "\n",
        "gnn = (\n",
        "    GraphWaveNet(\n",
        "        in_channels=memory_dim,\n",
        "        out_channels=embedding_dim,\n",
        "        msg_dim=data.msg.size(-1),\n",
        "        time_enc=memory.time_enc,\n",
        "    )\n",
        "    .to(device)\n",
        "    .float()\n",
        ")\n",
        "\n",
        "node_pred = NodePredictor(in_dim=embedding_dim, out_dim=num_classes).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    set(memory.parameters()) | set(gnn.parameters()) | set(node_pred.parameters()),\n",
        "    lr=lr,\n",
        ")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Helper vector to map global node indices to local ones.\n",
        "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "def plot_curve(scores, out_name):\n",
        "    plt.plot(scores, color=\"#e34a33\")\n",
        "    plt.ylabel(\"score\")\n",
        "    plt.savefig(out_name + \".pdf\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def process_edges(src, dst, t, msg):\n",
        "    if src.nelement() > 0:\n",
        "        # msg = msg.to(torch.float32)\n",
        "        memory.update_state(src, dst, t, msg)\n",
        "        neighbor_loader.insert(src, dst)\n",
        "\n",
        "\n",
        "def train():\n",
        "    memory.train()\n",
        "    gnn.train()\n",
        "    node_pred.train()\n",
        "\n",
        "    memory.reset_state()  # Start with a fresh memory.\n",
        "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
        "\n",
        "    total_loss = 0\n",
        "    label_t = dataset.get_label_time()  # check when does the first label start\n",
        "    num_label_ts = 0\n",
        "    total_score = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        src, dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        query_t = batch.t[-1]\n",
        "        # check if this batch moves to the next day\n",
        "        if query_t > label_t:\n",
        "            # find the node labels from the past day\n",
        "            label_tuple = dataset.get_node_label(query_t)\n",
        "            label_ts, label_srcs, labels = (\n",
        "                label_tuple[0],\n",
        "                label_tuple[1],\n",
        "                label_tuple[2],\n",
        "            )\n",
        "            label_t = dataset.get_label_time()\n",
        "            label_srcs = label_srcs.to(device)\n",
        "\n",
        "            # Process all edges that are still in the past day\n",
        "            previous_day_mask = batch.t < label_t\n",
        "            process_edges(\n",
        "                src[previous_day_mask],\n",
        "                dst[previous_day_mask],\n",
        "                t[previous_day_mask],\n",
        "                msg[previous_day_mask],\n",
        "            )\n",
        "            # Reset edges to be the edges from tomorrow so they can be used later\n",
        "            src, dst, t, msg = (\n",
        "                src[~previous_day_mask],\n",
        "                dst[~previous_day_mask],\n",
        "                t[~previous_day_mask],\n",
        "                msg[~previous_day_mask],\n",
        "            )\n",
        "\n",
        "            \"\"\"\n",
        "            modified for node property prediction\n",
        "            1. sample neighbors from the neighbor loader for all nodes to be predicted\n",
        "            2. extract memory from the sampled neighbors and the nodes\n",
        "            3. run gnn with the extracted memory embeddings and the corresponding time and message\n",
        "            \"\"\"\n",
        "            n_id = label_srcs\n",
        "            n_id_neighbors, mem_edge_index, e_id = neighbor_loader(n_id)\n",
        "            assoc[n_id_neighbors] = torch.arange(n_id_neighbors.size(0), device=device)\n",
        "\n",
        "            z, last_update = memory(n_id_neighbors)\n",
        "\n",
        "            z = gnn(\n",
        "                z,\n",
        "                last_update,\n",
        "                mem_edge_index,\n",
        "                data.t[e_id].to(device),\n",
        "                data.msg[e_id].to(device),\n",
        "            )\n",
        "            z = z[assoc[n_id]]\n",
        "\n",
        "            # loss and metric computation\n",
        "            pred = node_pred(z)\n",
        "\n",
        "            loss = criterion(pred, labels.to(device))\n",
        "            np_pred = pred.cpu().detach().numpy()\n",
        "            np_true = labels.cpu().detach().numpy()\n",
        "\n",
        "            input_dict = {\n",
        "                \"y_true\": np_true,\n",
        "                \"y_pred\": np_pred,\n",
        "                \"eval_metric\": [eval_metric],\n",
        "            }\n",
        "            result_dict = evaluator.eval(input_dict)\n",
        "            score = result_dict[eval_metric]\n",
        "            total_score += score\n",
        "            num_label_ts += 1\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss)\n",
        "\n",
        "        # Update memory and neighbor loader with ground-truth state.\n",
        "        process_edges(src, dst, t, msg)\n",
        "        memory.detach()\n",
        "\n",
        "    metric_dict = {\n",
        "        \"ce\": total_loss / num_label_ts,\n",
        "    }\n",
        "    metric_dict[eval_metric] = total_score / num_label_ts\n",
        "    return metric_dict\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    memory.eval()\n",
        "    gnn.eval()\n",
        "    node_pred.eval()\n",
        "    total_score = 0\n",
        "    label_t = dataset.get_label_time()  # check when does the first label start\n",
        "    num_label_ts = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        src, dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        query_t = batch.t[-1]\n",
        "        if query_t > label_t:\n",
        "            label_tuple = dataset.get_node_label(query_t)\n",
        "            if label_tuple is None:\n",
        "                break\n",
        "            label_ts, label_srcs, labels = (\n",
        "                label_tuple[0],\n",
        "                label_tuple[1],\n",
        "                label_tuple[2],\n",
        "            )\n",
        "            label_t = dataset.get_label_time()\n",
        "            label_srcs = label_srcs.to(device)\n",
        "\n",
        "            # Process all edges that are still in the past day\n",
        "            previous_day_mask = batch.t < label_t\n",
        "            process_edges(\n",
        "                src[previous_day_mask],\n",
        "                dst[previous_day_mask],\n",
        "                t[previous_day_mask],\n",
        "                msg[previous_day_mask],\n",
        "            )\n",
        "            # Reset edges to be the edges from tomorrow so they can be used later\n",
        "            src, dst, t, msg = (\n",
        "                src[~previous_day_mask],\n",
        "                dst[~previous_day_mask],\n",
        "                t[~previous_day_mask],\n",
        "                msg[~previous_day_mask],\n",
        "            )\n",
        "\n",
        "            \"\"\"\n",
        "            modified for node property prediction\n",
        "            1. sample neighbors from the neighbor loader for all nodes to be predicted\n",
        "            2. extract memory from the sampled neighbors and the nodes\n",
        "            3. run gnn with the extracted memory embeddings and the corresponding time and message\n",
        "            \"\"\"\n",
        "            n_id = label_srcs\n",
        "            n_id_neighbors, mem_edge_index, e_id = neighbor_loader(n_id)\n",
        "            assoc[n_id_neighbors] = torch.arange(n_id_neighbors.size(0), device=device)\n",
        "\n",
        "            z, last_update = memory(n_id_neighbors)\n",
        "            z = gnn(\n",
        "                z,\n",
        "                last_update,\n",
        "                mem_edge_index,\n",
        "                data.t[e_id].to(device),\n",
        "                data.msg[e_id].to(device),\n",
        "            )\n",
        "            z = z[assoc[n_id]]\n",
        "\n",
        "            # loss and metric computation\n",
        "            pred = node_pred(z)\n",
        "            np_pred = pred.cpu().detach().numpy()\n",
        "            np_true = labels.cpu().detach().numpy()\n",
        "\n",
        "            input_dict = {\n",
        "                \"y_true\": np_true,\n",
        "                \"y_pred\": np_pred,\n",
        "                \"eval_metric\": [eval_metric],\n",
        "            }\n",
        "            result_dict = evaluator.eval(input_dict)\n",
        "            score = result_dict[eval_metric]\n",
        "            total_score += score\n",
        "            num_label_ts += 1\n",
        "\n",
        "        process_edges(src, dst, t, msg)\n",
        "\n",
        "    metric_dict = {}\n",
        "    metric_dict[eval_metric] = total_score / num_label_ts\n",
        "    return metric_dict\n",
        "\n",
        "\n",
        "train_curve = []\n",
        "val_curve = []\n",
        "test_curve = []\n",
        "max_val_score = 0  #find the best test score based on validation score\n",
        "best_test_idx = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    start_time = timeit.default_timer()\n",
        "    train_dict = train()\n",
        "    print(\"------------------------------------\")\n",
        "    print(f\"training Epoch: {epoch:02d}\")\n",
        "    print(train_dict)\n",
        "    train_curve.append(train_dict[eval_metric])\n",
        "    print(\"Training takes--- %s seconds ---\" % (timeit.default_timer() - start_time))\n",
        "    \n",
        "    start_time = timeit.default_timer()\n",
        "    val_dict = test(val_loader)\n",
        "    print(val_dict)\n",
        "    val_curve.append(val_dict[eval_metric])\n",
        "    if (val_dict[eval_metric] > max_val_score):\n",
        "        max_val_score = val_dict[eval_metric]\n",
        "        best_test_idx = epoch - 1\n",
        "    print(\"Validation takes--- %s seconds ---\" % (timeit.default_timer() - start_time))\n",
        "\n",
        "    start_time = timeit.default_timer()\n",
        "    test_dict = test(test_loader)\n",
        "    print(test_dict)\n",
        "    test_curve.append(test_dict[eval_metric])\n",
        "    print(\"Test takes--- %s seconds ---\" % (timeit.default_timer() - start_time))\n",
        "    print(\"------------------------------------\")\n",
        "    dataset.reset_label_time()\n",
        "\n",
        "\n",
        "# code for plotting\n",
        "plot_curve(train_curve, \"train_curve\")\n",
        "plot_curve(val_curve, \"val_curve\")\n",
        "plot_curve(test_curve, \"test_curve\")\n",
        "\n",
        "max_test_score = test_curve[best_test_idx]\n",
        "print(\"------------------------------------\")\n",
        "print(\"------------------------------------\")\n",
        "print (\"best val score: \", max_val_score)\n",
        "print (\"best validation epoch   : \", best_test_idx + 1)\n",
        "print (\"best test score: \", max_test_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
